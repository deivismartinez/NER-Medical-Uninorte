{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_md"
      ],
      "metadata": {
        "id": "7jNIcyoFAZlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "import json\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import spacy\n",
        "from matplotlib import pyplot as plt\n",
        "from math import ceil\n",
        "import shutil"
      ],
      "metadata": {
        "id": "Bo6h_ZufAWcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/4279323/files/meddocan.zip?download=1 -O meddocan.zip"
      ],
      "metadata": {
        "id": "2q1unnwOU8_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip meddocan.zip"
      ],
      "metadata": {
        "id": "k3MKJGmLU_Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"es_core_news_md\")"
      ],
      "metadata": {
        "id": "uZcj5ytyAR99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {}\n",
        "for path in (Path('dev'), Path('test'), Path('train')):\n",
        "    dir_path = Path('meddocan')/path/Path('brat')\n",
        "    filenames = tuple(f[:-4] for f in listdir(dir_path) if isfile(join(dir_path, f)) if f[-4:] == '.txt')\n",
        "    dataset[str(path)] = []\n",
        "    for file_name in filenames:\n",
        "      d = dict()\n",
        "      with open(dir_path/Path(file_name+'.txt'), 'r') as f:\n",
        "        dataset[str(path)].append({\"text\":f.read(),\"file\":file_name,\"file_name_path\":dir_path/Path(file_name+'.txt')})"
      ],
      "metadata": {
        "id": "pdwA4hf_VgEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/deivismartinez/NER-Medical-Uninorte/main/tag.json"
      ],
      "metadata": {
        "id": "xkXzSk_sSfdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "  models = [\n",
        "    {\"id\":1,\"name\":\"roberta-large-bne-capitel-ner\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":2,\"name\":\"roberta-base-bne-capitel-ner-plus\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":3,\"name\":\"roberta-base-bne-capitel-ner\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":5,\"name\":\"bsc-bio-ehr-es-pharmaconer\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":6,\"name\":\"roberta-base-bne-capitel-pos\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":7,\"name\":\"bert-base-multilingual-cased-ner-hrl\", \"folder\":\"Davlan/\"},\n",
        "    {\"id\":8,\"name\":\"xlm-roberta-base-wikiann-ner\", \"folder\":\"Davlan/\"},\n",
        "    {\"id\":9,\"name\":\"xlm-roberta-base-ner-hrl\", \"folder\":\"Davlan/\"},\n",
        "    {\"id\":10,\"name\":\"distilbert-base-multilingual-cased-ner-hrl\", \"folder\":\"Davlan/\"},\n",
        "    {\"id\":11,\"name\":\"xlm-roberta-large-ner-hrl\", \"folder\":\"Davlan/\"},\n",
        "    {\"id\":12,\"name\":\"bert-spanish-cased-finetuned-ner\", \"folder\":\"mrm8488/\"},\n",
        "    {\"id\":13,\"name\":\"TinyBERT-spanish-uncased-finetuned-ner\", \"folder\":\"mrm8488/\"},\n",
        "    {\"id\":14,\"name\":\"RuPERTa-base-finetuned-ner\", \"folder\":\"mrm8488/\"},\n",
        "    {\"id\":15,\"name\":\"ner-spanish-large\", \"folder\":\"flair/\"},\n",
        "    {\"id\":16,\"name\":\"ner-multi\", \"folder\":\"flair/\"},\n",
        "    {\"id\":17,\"name\":\"anglicisms-spanish-mbert\", \"folder\":\"lirondos/\"},\n",
        "    {\"id\":18,\"name\":\"anglicisms-spanish-flair-cs\", \"folder\":\"lirondos/\"},\n",
        "    {\"id\":19,\"name\":\"xlm-roberta-large-ner-spanish\", \"folder\":\"MMG/\"},\n",
        "    {\"id\":20,\"name\":\"wikineural-multilingual-ner\", \"folder\":\"Babelscape/\"},\n",
        "    {\"id\":21,\"name\":\"distilbert-base-multilingual-cased-finetuned-conll2003-ner\", \"folder\":\"gunghio/\"},\n",
        "    {\"id\":22,\"name\":\"bertin-base-ner-conll2002-es\", \"folder\":\"bertin-project/\"},\n",
        "    {\"id\":23,\"name\":\"xlm-roberta-base-finetuned-panx-ner\", \"folder\":\"gunghio/\"},\n",
        "    {\"id\":24,\"name\":\"stanza-es · Hugging Face\", \"folder\":\"stanfordnlp/\"},\n",
        "    ]\n",
        "  return models"
      ],
      "metadata": {
        "id": "fRt1_zo8GnSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_model(model_index):\n",
        "  models = get_models()\n",
        "  if model_index > len(models):\n",
        "    #print(\"Model index not exist, from 1 to \"+str(len(models)))\n",
        "    return None\n",
        "  else:\n",
        "    model = models[model_index-1].get(\"folder\") + models[model_index-1].get(\"name\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "1_xSXFOA63N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_models(folder_base):\n",
        "  models = get_models()\n",
        "  models_dict = {\"models\":models}\n",
        "  with open(folder_base+\"models.json\", \"w\") as fp:\n",
        "    json.dump(models_dict, fp)"
      ],
      "metadata": {
        "id": "r5scJQKkG7qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ner_pipe(model):\n",
        "  ner_pipe = pipeline(task=\"ner\", model = model)\n",
        "  return ner_pipe"
      ],
      "metadata": {
        "id": "wbJMT51z3MO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7E8mFQrrMB9"
      },
      "outputs": [],
      "source": [
        "file = open(\"tag.json\",\"r\")\n",
        "tag = json.load(file)\n",
        "file.close()\n",
        "def get_tag(entity):\n",
        "  tags=[\"LOC\",\"PER\",\"ORG\", \"OTH\"]\n",
        "  for tag_l in tags:\n",
        "    if tag_l in entity:\n",
        "      return tag[tag_l]\n",
        "  return \"NEW TAG\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(model):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model,add_prefix_space=True)\n",
        "  #do_grafic(tokenizer)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "6KdxtBtwYYUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_grafic(tokenizer):\n",
        "  token_length = [len(tokenizer(x.get('text'))['input_ids']) for x in dataset['test']]\n",
        "  plt.hist(token_length)"
      ],
      "metadata": {
        "id": "1hP-u5dtc7dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_start_end(list_values, n_string):\n",
        "  for entity in list_values:\n",
        "    entity[\"start\"] = entity.get(\"start\") + n_string\n",
        "    entity[\"end\"] = entity.get(\"end\") + n_string\n",
        "  return list_values"
      ],
      "metadata": {
        "id": "KqDha5mEcY-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_file_new(entity_list, file_name, min_score = 0.5):\n",
        "  t = 0\n",
        "  file= open(file_name,\"w\")\n",
        "  end_last = -1\n",
        "  entity_last = \"\"\n",
        "  word_last = \"\"\n",
        "  for entity in entity_list:\n",
        "      #print('--------------Entity Actual::: ',entity['entity'],'-----------',entity['word'].replace('Ġ',' ').replace('Ã±','ñ').replace('Ã¡','á').replace('Ã©','é').replace('ÃŃ','í').replace('Ã³','ó').replace('Ãº','ú'))\n",
        "      if entity['score'] > min_score:\n",
        "        if (entity['start'] == end_last or end_last == -1) or ((entity['start'] == (end_last+1)) \n",
        "        and get_tag(entity['entity']) == get_tag(entity_last)):\n",
        "          word_last += entity['word']\n",
        "          if end_last == -1:\n",
        "            start_first = entity['start']\n",
        "            entity_first = entity['entity']\n",
        "        else:\n",
        "          t += 1\n",
        "          end_now = entity['end']\n",
        "          entity_now = get_tag(entity_first)\n",
        "          row = 'T'+str(t) +'\\t'+ str(entity_now) +' ' + str(start_first) +' '+ str(end_last) +'\\t' + word_last.replace('Ġ',' ').replace('Ã±','ñ').replace('Ã¡','á').replace('Ã©','é').replace('ÃŃ','í').replace('Ã³','ó').replace('Ãº','ú') +'\\n'\n",
        "          if(entity_now!='OTHER'):\n",
        "            file.write(row)\n",
        "          word_last = entity['word']\n",
        "          start_first = entity['start']\n",
        "          entity_first = entity['entity']\n",
        "        end_last = entity['end']\n",
        "        entity_last = entity['entity']\n",
        "  t += 1\n",
        "  entity_now = get_tag(entity_first)\n",
        "  row = 'T'+str(t) +'\\t'+ str(entity_now) +' ' + str(start_first) +' '+ str(end_last) +'\\t' + word_last.replace('Ġ',' ').replace('Ã±','ñ').replace('Ã¡','á').replace('Ã©','é').replace('ÃŃ','í').replace('Ã³','ó').replace('Ãº','ú') +'\\n'\n",
        "  if(entity_now!='OTHER'):\n",
        "    file.write(row)\n",
        "  file.close ()"
      ],
      "metadata": {
        "id": "c18S3DUsSIlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_values_512(texto, ner_pipe, model_index, tokenizer):\n",
        "  max_length = 512\n",
        "  n_string = -1\n",
        "  ner_output = []\n",
        "  model = select_model(model_index)\n",
        "  if model != None:\n",
        "    #tokenizer = get_tokenizer(model)\n",
        "    tokens = tokenizer(texto)['input_ids']\n",
        "    if len(tokens) > max_length:\n",
        "      n_partitions = ceil(len(tokens) / max_length)\n",
        "      for i in range(n_partitions):\n",
        "        end_index = (i+1)*max_length\n",
        "        if end_index > len(tokens):\n",
        "          end_index  = len(tokens) - 1 \n",
        "        token_sequence = tokens[i*max_length:end_index]\n",
        "        string_sequence = tokenizer.decode(token_sequence, \n",
        "                                          skip_special_tokens=True,\n",
        "                                          clean_up_tokenization_spaces=False)\n",
        "        new_list = change_start_end(ner_pipe(string_sequence), n_string)\n",
        "        ner_output.extend(new_list)\n",
        "        n_string = n_string + len(string_sequence)\n",
        "      if n_string != len(texto):\n",
        "        print(f'Original length: {len(texto)} vs Decoded Size {n_string}')\n",
        "    else:\n",
        "      ner_output.extend(ner_pipe(texto))\n",
        "  return ner_output"
      ],
      "metadata": {
        "id": "Kec7jy_SdRlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_values_sents(nlp_var, ner_pipe):\n",
        "  entity_list =  []\n",
        "  sum = 0\n",
        "  #print(type(nlp_var))\n",
        "  for text_npl in nlp_var.sents:\n",
        "    for entity in ner_pipe(str(text_npl)):\n",
        "      entity[\"start\"] = entity.get(\"start\") + sum\n",
        "      entity[\"end\"] = entity.get(\"end\") + sum\n",
        "      entity_list.append(entity)\n",
        "    sum = sum + len(str(text_npl))\n",
        "  return entity_list"
      ],
      "metadata": {
        "id": "-oQ4eSm01KDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_values_split(text, ner_pipe):\n",
        "  entity_list =  []\n",
        "  text_split = text.split('.')\n",
        "  sum = 0\n",
        "  for text_npl in text_split:\n",
        "    #print(text_npl)\n",
        "    for entity in ner_pipe(str(text_npl)):\n",
        "      entity[\"start\"] = entity.get(\"start\") + sum\n",
        "      entity[\"end\"] = entity.get(\"end\") + sum\n",
        "      entity_list.append(entity)\n",
        "    sum = sum + len(str(text_npl))\n",
        "  return entity_list"
      ],
      "metadata": {
        "id": "sY5XYlXSjXeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_file(text, file_name, folder_system, min_score,\n",
        "                 type_text, ner_pipe, model_index, tokenizer):\n",
        "  file_ann_name = folder_system + file_name + \".ann\"\n",
        "  nlp_var = nlp(text)\n",
        "  entity_list = []\n",
        "  if type_text == 0:\n",
        "    entity_list = ner_pipe(str(nlp_var))\n",
        "  elif type_text == 1:\n",
        "    entity_list = get_values_sents(nlp_var, ner_pipe = ner_pipe)\n",
        "  elif type_text == 2:\n",
        "    entity_list = get_values_512(text, ner_pipe = ner_pipe,\n",
        "                                 model_index=model_index, tokenizer=tokenizer)\n",
        "  elif type_text == 3:\n",
        "    entity_list = get_values_split(text, ner_pipe = ner_pipe)\n",
        "  else:\n",
        "    print(\"Type not exist\")\n",
        "  if len(entity_list) > 0:\n",
        "    build_file_new(entity_list,file_ann_name, min_score = min_score)"
      ],
      "metadata": {
        "id": "MUk9bJDioFaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def builder(min_score, quantity = 1, type_text = 0, model_index = 1):\n",
        "  model = select_model(model_index)\n",
        "  if model != None:\n",
        "    ner_pipe = get_ner_pipe(model)\n",
        "    tokenizer = get_tokenizer(model)\n",
        "    folder_base = \"system/\"\n",
        "    folder_system = folder_base+\"model_\"+str(model_index)+\"/test\"+str(type_text)+\"/\"\n",
        "    folder_gold = folder_base+\"gold/\"\n",
        "    if not Path(folder_system).exists():\n",
        "      path = Path(folder_system)\n",
        "      path.mkdir(parents=True)\n",
        "    if not Path(folder_gold).exists():\n",
        "      path = Path(folder_gold)\n",
        "      path.mkdir(parents=True)\n",
        "    save_models(folder_base)\n",
        "    for text in dataset['test']:\n",
        "      #if text.get('file') == 'S0211-69952014000600016-1':\n",
        "      path_completo = str(text.get('file_name_path'))\n",
        "      path_completo = path_completo[:-4]\n",
        "      prepare_file(text.get('text'), text.get('file'), folder_system,\n",
        "                  min_score = min_score, type_text = type_text,\n",
        "                  ner_pipe = ner_pipe, model_index = model_index, tokenizer = tokenizer)\n",
        "      shutil.copy(path_completo+\".txt\", folder_gold+text.get('file')+\".txt\")\n",
        "      shutil.copy(path_completo+\".ann\", folder_gold+text.get('file')+\".ann\")\n",
        "      shutil.copy(path_completo+\".txt\", folder_system+text.get('file')+\".txt\")\n",
        "      quantity -= 1\n",
        "      if quantity < 1:\n",
        "        break"
      ],
      "metadata": {
        "id": "31sQxVUj_vfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def builder_all():\n",
        "  for i in range(0,4):\n",
        "    for j in range(1,3):\n",
        "      builder(type_text = i, min_score = 0.0, quantity = 250, model_index = j)"
      ],
      "metadata": {
        "id": "Efb-XLWWSeY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder_all()"
      ],
      "metadata": {
        "id": "PeSpSykoUN6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder(type_text = 3, min_score = 0.0, quantity = 1, model_index = 2)"
      ],
      "metadata": {
        "id": "rk1wNaMojqmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Las tablas finales con el de test y desarrollo con train\n",
        "builder(type_text = 2, min_score = 0.0, quantity = 1, model_index = 2)"
      ],
      "metadata": {
        "id": "ONgas9UL4A_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder(type_text = 1, min_score = 0.0, quantity = 1, model_index = 2)"
      ],
      "metadata": {
        "id": "ByIg6BUR6etL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder(type_text = 0, min_score = 0.0, quantity = 1, model_index = 2)"
      ],
      "metadata": {
        "id": "t6rjULHG6hZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "fantasy_zip = zipfile.ZipFile('/content/systemd.zip', 'w')\n",
        "for folder, subfolders, files in os.walk('/content/system'): \n",
        "    for file in files:\n",
        "        if file.endswith('.ann') or file.endswith('.txt'):\n",
        "            fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), '/content/system'), compress_type = zipfile.ZIP_DEFLATED)\n",
        "fantasy_zip.close()"
      ],
      "metadata": {
        "id": "nf4cmHj45e6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcyyW-2Thhu6"
      },
      "outputs": [],
      "source": [
        "def remove_file():\n",
        "  from pathlib import Path\n",
        "  folder_system = \"/content/system/\"\n",
        "  folder = Path(folder_system)\n",
        "  for file in folder.iterdir():\n",
        "    try:\n",
        "      file_name = file.name[0:len(file.name)-4]\n",
        "      filePath=Path(folder_system+file_name+\".ann\")\n",
        "      filePath.unlink()\n",
        "    except OSError as e:\n",
        "      print(f\"Error:{ e.strerror}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}