{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4aw8IiGrO4LCTdBJVdLiG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deivismartinez/NER-Medical-Uninorte/blob/main/NER2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi5sCd__MswA"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers\n",
        "!pip install --no-cache-dir transformers sentencepiece\n",
        "#!pip install spacy\n",
        "#!python -m spacy download es_core_news_md\n",
        "#!pip install flair\n",
        "#!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "from transformers.models.bert.modeling_bert import BertModel,BertForMaskedLM\n",
        "import json\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "#import spacy\n",
        "from matplotlib import pyplot as plt\n",
        "from math import ceil\n",
        "import shutil\n",
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "#import stanza\n",
        "import sys"
      ],
      "metadata": {
        "id": "83t25t-2M_j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/4279323/files/meddocan.zip?download=1 -O meddocan.zip"
      ],
      "metadata": {
        "id": "ztut1D9_NrJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip meddocan.zip"
      ],
      "metadata": {
        "id": "QxbDsGnPN2Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {}\n",
        "for path in (Path('dev'), Path('test'), Path('train')):\n",
        "    dir_path = Path('meddocan')/path/Path('brat')\n",
        "    filenames = tuple(f[:-4] for f in listdir(dir_path) if isfile(join(dir_path, f)) if f[-4:] == '.txt')\n",
        "    dataset[str(path)] = []\n",
        "    for file_name in filenames:\n",
        "      d = dict()\n",
        "      with open(dir_path/Path(file_name+'.txt'), 'r') as f:\n",
        "        dataset[str(path)].append({\"text\":f.read(),\"file\":file_name,\"file_name_path\":dir_path/Path(file_name+'.txt')})"
      ],
      "metadata": {
        "id": "KQq_9YlcOeya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/deivismartinez/NER-Medical-Uninorte/main/tag.json\n",
        "!wget https://raw.githubusercontent.com/deivismartinez/NER-Medical-Uninorte/main/models.json"
      ],
      "metadata": {
        "id": "DkWRAjkIOleQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "  #file = open(\"models.json\",\"r\")\n",
        "  #models = json.load(file)\n",
        "  #file.close()\n",
        "  models = [\n",
        "    {\"id\":1,\"name\":\"roberta-large-NER\", \"folder\":\"51la5/\"},\n",
        "    {\"id\":2,\"name\":\"bert-base-multilingual-cased-ner-spanish\", \"folder\":\"alvarobartt/\"},\n",
        "    {\"id\":3,\"name\":\"anonymizer-beto-cased-flair\", \"folder\":\"aymurai/\"},\n",
        "    {\"id\":4,\"name\":\"flair-ner-spanish-judicial\", \"folder\":\"aymurai/\"},\n",
        "    {\"id\":5,\"name\":\"wikineural-multilingual-ner\", \"folder\":\"Babelscape/\"},\n",
        "    {\"id\":6,\"name\":\"bertin-base-ner-conll2002-es\", \"folder\":\"bertin-project/\"},\n",
        "    {\"id\":7,\"name\":\"roberta_model_for_anonimization\", \"folder\":\"BSC-LT/\"},\n",
        "    {\"id\":8,\"name\":\"roberta-base-bne-capitel-ner-plus\", \"folder\":\"BSC-LT/\"},\n",
        "    {\"id\":9,\"name\":\"NER-MEDDOCAN\", \"folder\":\"Dnidof/\"},\n",
        "    {\"id\":10,\"name\":\"bert-base-multilingual-cased-fine_tuned-ner-WikiNeural_Multilingual\", \"folder\":\"DunnBC22/\"},\n",
        "    {\"id\":11,\"name\":\"xlm-roberta-large-finetuned-conll03-english\", \"folder\":\"FacebookAI/\"},\n",
        "    {\"id\":12,\"name\":\"xlm-roberta-large-finetuned-conll03-german\", \"folder\":\"FacebookAI/\"},\n",
        "    {\"id\":13,\"name\":\"ner-multi\", \"folder\":\"flair/\"},\n",
        "    {\"id\":14,\"name\":\"ner-multi-fast\", \"folder\":\"flair/\"},\n",
        "    {\"id\":15,\"name\":\"ner-spanish-large\", \"folder\":\"flair/\"},\n",
        "    {\"id\":16,\"name\":\"AuthorParserModel\", \"folder\":\"GEOcite/\"},\n",
        "    {\"id\":17,\"name\":\"distilbert-base-multilingual-cased-finetuned-conll2003-ner\", \"folder\":\"gunghio/\"},\n",
        "    {\"id\":18,\"name\":\"xlm-roberta-base-finetuned-panx-ner\", \"folder\":\"gunghio/\"},\n",
        "    {\"id\":19,\"name\":\"BETO-finetuned-ner-3\", \"folder\":\"ifis/\"},\n",
        "    {\"id\":20,\"name\":\"span-marker-bert-base-multilingual-cased-multinerd\", \"folder\":\"lxyuan/\"},\n",
        "    {\"id\":21,\"name\":\"span-marker-bert-base-multilingual-uncased-multinerd\", \"folder\":\"lxyuan/\"},\n",
        "    {\"id\":22,\"name\":\"xlm-roberta-large-ner-spanish\", \"folder\":\"MMG/\"},\n",
        "    {\"id\":23,\"name\":\"bert-spanish-cased-finetuned-ner\", \"folder\":\"mrm8488/\"},\n",
        "    {\"id\":24,\"name\":\"RuPERTa-base-finetuned-ner\", \"folder\":\"mrm8488/\"},\n",
        "    {\"id\":25,\"name\":\"TinyBERT-spanish-uncased-finetuned-ner\", \"folder\":\"mrm8488/\"},\n",
        "    {\"id\":26,\"name\":\"NER-fine-tuned-BETO\", \"folder\":\"NazaGara/\"},\n",
        "    {\"id\":27,\"name\":\"flair-ner-multi\", \"folder\":\"Omnifact/\"},\n",
        "    {\"id\":28,\"name\":\"ca_anonimization_core_lg\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":29,\"name\":\"es_anonimization_core_lg\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":30,\"name\":\"roberta-base-bne-capitel-ner\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":31,\"name\":\"roberta-base-bne-capitel-ner-plus\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":32,\"name\":\"roberta-large-bne-capitel-ner\", \"folder\":\"PlanTL-GOB-ES/\"},\n",
        "    {\"id\":33,\"name\":\"DEBERTA_CIEL\", \"folder\":\"projecte-aina/\"},\n",
        "    {\"id\":34,\"name\":\"codeswitch-spaeng-ner-lince\", \"folder\":\"sagorsarker/\"},\n",
        "    {\"id\":35,\"name\":\"span-marker-bert-base-conll2002-es\", \"folder\":\"sepulm01/\"},\n",
        "    {\"id\":36,\"name\":\"es_core_news_lg\", \"folder\":\"spacy/\"},\n",
        "    {\"id\":37,\"name\":\"es_core_news_md\", \"folder\":\"spacy/\"},\n",
        "    {\"id\":38,\"name\":\"es_core_news_sm\", \"folder\":\"spacy/\"},\n",
        "    {\"id\":39,\"name\":\"stanza-es\", \"folder\":\"stanfordnlp/\"},\n",
        "    {\"id\":40,\"name\":\"gliner_multi_pii-v1\", \"folder\":\"urchade/\"},\n",
        "    ]\n",
        "  return models\n",
        "\n",
        "def select_model(model_index):\n",
        "  models = get_models()\n",
        "  if model_index > len(models):\n",
        "    return None\n",
        "  else:\n",
        "    model = models[model_index-1].get(\"folder\") + models[model_index-1].get(\"name\")\n",
        "    return model\n",
        "\n",
        "def save_models(folder_base):\n",
        "  models = get_models()\n",
        "  models_dict = {\"models\":models}\n",
        "  with open(folder_base+\"models.json\", \"w\") as fp:\n",
        "    json.dump(models_dict, fp)\n",
        "\n",
        "def get_ner_pipe(model):\n",
        "  ner_pipe = pipeline(task=\"ner\", model = model)\n",
        "  return ner_pipe"
      ],
      "metadata": {
        "id": "KA8aV7ul2pHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"tag.json\",\"r\")\n",
        "tag = json.load(file)\n",
        "file.close()\n",
        "def get_tag(entity):\n",
        "  tags=[\"LOC\",\"PER\",\"ORG\", \"OTH\", \"MISC\"]\n",
        "  for tag_l in tags:\n",
        "    if tag_l in entity:\n",
        "      return tag[tag_l]\n",
        "  return \"NEW TAG \""
      ],
      "metadata": {
        "id": "Mmt7wTTz8R-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(model):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model,add_prefix_space=True)\n",
        "  #do_grafic(tokenizer)\n",
        "  return tokenizer\n",
        "\n",
        "def do_grafic(tokenizer):\n",
        "  token_length = [len(tokenizer(x.get('text'))['input_ids']) for x in dataset['test']]\n",
        "  plt.hist(token_length)\n",
        "\n",
        "def change_start_end(list_values, n_string):\n",
        "  for entity in list_values:\n",
        "    entity[\"start\"] = entity.get(\"start\") + n_string\n",
        "    entity[\"end\"] = entity.get(\"end\") + n_string\n",
        "  return list_values\n",
        "\n",
        "def build_file(entity_list, file_name, min_score = 0.5):\n",
        "  t = 0\n",
        "  file= open(file_name,\"w\")\n",
        "  end_last = -1\n",
        "  entity_last = \"\"\n",
        "  word_last = \"\"\n",
        "  for entity in entity_list:\n",
        "      if entity['score'] > min_score:\n",
        "        if (entity['start'] == end_last or end_last == -1) or ((entity['start'] == (end_last+1))\n",
        "        and get_tag(entity['entity']) == get_tag(entity_last)):\n",
        "          if (entity['start'] == end_last or end_last == -1):\n",
        "            word_last += entity['word']\n",
        "          else:\n",
        "            word_last += ' ' + entity['word']\n",
        "          if end_last == -1:\n",
        "            start_first = entity['start']\n",
        "            entity_first = entity['entity']\n",
        "        else:\n",
        "          t += 1\n",
        "          end_now = entity['end']\n",
        "          entity_now = get_tag(entity_first)\n",
        "          row = 'T'+str(t) +'\\t'+ str(entity_now) +' ' + str(start_first) +' '+ str(end_last) +'\\t' + word_last.replace('Ġ',' ').replace('Ã±','ñ').replace('Ã¡','á').replace('Ã©','é').replace('ÃŃ','í').replace('Ã³','ó').replace('Ãº','ú').replace('▁',' ').replace('##','') +'\\n'\n",
        "          if(entity_now!='OTHER'):\n",
        "            file.write(row)\n",
        "          word_last = entity['word']\n",
        "          start_first = entity['start']\n",
        "          entity_first = entity['entity']\n",
        "        end_last = entity['end']\n",
        "        entity_last = entity['entity']\n",
        "  t += 1\n",
        "  entity_now = get_tag(entity_first)\n",
        "  row = 'T'+str(t) +'\\t'+ str(entity_now) +' ' + str(start_first) +' '+ str(end_last) +'\\t' + word_last.replace('Ġ',' ').replace('Ã±','ñ').replace('Ã¡','á').replace('Ã©','é').replace('ÃŃ','í').replace('Ã³','ó').replace('Ãº','ú').replace('▁',' ').replace('##','') +'\\n'\n",
        "  if(entity_now!='OTHER'):\n",
        "    file.write(row)\n",
        "  file.close ()\n",
        "\n",
        "def get_values_sents(nlp_var, ner_pipe):\n",
        "  entity_list =  []\n",
        "  sum = 0\n",
        "  #print(type(nlp_var))\n",
        "  for text_npl in nlp_var.sents:\n",
        "    for entity in ner_pipe(str(text_npl)):\n",
        "      entity[\"start\"] = entity.get(\"start\") + sum\n",
        "      entity[\"end\"] = entity.get(\"end\") + sum\n",
        "      entity_list.append(entity)\n",
        "    sum = sum + len(str(text_npl))\n",
        "  return entity_list\n",
        "\n",
        "def get_values_split(text, ner_pipe):\n",
        "  entity_list =  []\n",
        "  text_split = text.split('.')\n",
        "  sum = 0\n",
        "  for text_npl in text_split:\n",
        "    #print(text_npl)\n",
        "    for entity in ner_pipe(str(text_npl)):\n",
        "      entity[\"start\"] = entity.get(\"start\") + sum\n",
        "      entity[\"end\"] = entity.get(\"end\") + sum\n",
        "      entity_list.append(entity)\n",
        "    sum = sum + len(str(text_npl))\n",
        "  return entity_list\n",
        "\n",
        "def get_values_512(text, ner_pipe, model_index, tokenizer):\n",
        "  chunk_size = 512\n",
        "  entity_list = []\n",
        "  model = select_model(model_index)\n",
        "  if model != None:\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunk = text[i: i + chunk_size]\n",
        "        valors = ner_pipe(chunk)\n",
        "        valors = change_start_end(valors, i)\n",
        "        entity_list.extend(valors)\n",
        "  return entity_list\n",
        "\n",
        "def prepare_file(text, file_name, folder_system, min_score,\n",
        "                 type_text, ner_pipe, model_index, tokenizer):\n",
        "  file_ann_name = folder_system + file_name + \".ann\"\n",
        "  entity_list = []\n",
        "  if type_text == 0:\n",
        "    nlp_var = nlp(text)\n",
        "    entity_list = ner_pipe(str(nlp_var))\n",
        "  elif type_text == 1:\n",
        "    nlp_var = nlp(text)\n",
        "    entity_list = get_values_sents(nlp_var, ner_pipe = ner_pipe)\n",
        "  elif type_text == 2:\n",
        "    entity_list = get_values_512(text, ner_pipe = ner_pipe,\n",
        "                                 model_index=model_index, tokenizer=tokenizer) #para los casos atipicos\n",
        "  elif type_text == 3:\n",
        "    entity_list = get_values_split(text, ner_pipe = ner_pipe)\n",
        "  else:\n",
        "    print(\"Type not exist\")\n",
        "  if len(entity_list) > 0:\n",
        "    build_file(entity_list,file_ann_name, min_score = min_score)\n",
        "\n",
        "def builder(min_score, quantity = 1, type_text = 0, model_index = 1, model_type = 1):\n",
        "  model = select_model(model_index)\n",
        "  if model != None:\n",
        "    tokenizer = get_tokenizer(model)\n",
        "    ner_pipe = get_ner_pipe(model)\n",
        "\n",
        "    folder_base = \"system/\"\n",
        "    folder_system = folder_base+\"model_\"+str(model_index)+\"/test\"+str(type_text)+\"/\"\n",
        "    folder_gold = folder_base+\"gold/\"\n",
        "    if not Path(folder_system).exists():\n",
        "      path = Path(folder_system)\n",
        "      path.mkdir(parents=True)\n",
        "    if not Path(folder_gold).exists():\n",
        "      path = Path(folder_gold)\n",
        "      path.mkdir(parents=True)\n",
        "    save_models(folder_base)\n",
        "    for text in dataset['test']:\n",
        "        try:\n",
        "          path_completo = str(text.get('file_name_path'))\n",
        "          path_completo = path_completo[:-4]\n",
        "          prepare_file(text.get('text'), text.get('file'), folder_system,\n",
        "                      min_score = min_score, type_text = type_text,\n",
        "                      ner_pipe = ner_pipe, model_index = model_index, tokenizer = tokenizer)\n",
        "          shutil.copy(path_completo+\".txt\", folder_gold+text.get('file')+\".txt\")\n",
        "          shutil.copy(path_completo+\".ann\", folder_gold+text.get('file')+\".ann\")\n",
        "          shutil.copy(path_completo+\".txt\", folder_system+text.get('file')+\".txt\")\n",
        "        except:\n",
        "          print(f\"NO PROCESADO   -   ----   --   {text.get('file')}\")\n",
        "        quantity -= 1\n",
        "        print(quantity)\n",
        "        if quantity < 1:\n",
        "          break\n",
        "\n",
        "def builder_all():\n",
        "  for i in range(0,4):\n",
        "    for j in range(1,25):\n",
        "      builder(type_text = i, min_score = 0.0, quantity = 1, model_index = j)\n",
        "\n",
        "#Type: {'1':'Normal Transformer','2':'span_marker'}\n",
        "def builder_all_test(start_model = 1, end_model = 1, text = 2, quantity_test = 1):\n",
        "  for number_model in range(start_model,end_model):\n",
        "    builder(min_score = 0.0, quantity = quantity_test, type_text = text, model_index = number_model)"
      ],
      "metadata": {
        "id": "PEocUK--8YhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder_all_test(start_model = 1, end_model = 2, text = 2, quantity_test = 1)"
      ],
      "metadata": {
        "id": "62Xmy9gz8mI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def comprimir():\n",
        "  fantasy_zip = zipfile.ZipFile('/content/system.zip', 'w')\n",
        "  for folder, subfolders, filesFolder in os.walk('/content/system'):\n",
        "      for file in filesFolder:\n",
        "          if file.endswith('.conll') or file.endswith('.txt') or file.endswith('.ann'):\n",
        "              fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), '/content/system'), compress_type = zipfile.ZIP_DEFLATED)\n",
        "  fantasy_zip.close()"
      ],
      "metadata": {
        "id": "emQErBM__yfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comprimir()"
      ],
      "metadata": {
        "id": "jto_jm4uI7se"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}